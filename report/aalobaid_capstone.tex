\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{color}
\usepackage{listings}
%\newcommand{\ahmad}[2][inline]{\color{orange} [Ahmad]: #2\color{black}}
\newcommand{\comm}[2][inline]{\color{red}#2\color{black}}
\title{Capstone Project: TADA: a TAbular DAta classification of soft clusters using the Semantic Web}
\author{Ahmad Alobaid}
\date{\today}
 
\renewcommand*\contentsname{Content}
 
\begin{document}
 
\maketitle
 
\tableofcontents

\clearpage
\section{Definition (1-2 pages)}

\subsection{Project Overview}
In this century, people are storing and gather data more than ever in human history. We have massive amount of data scattered in different places. Before computers, information are kept in books and people used to look for information in the library, were a lot of books are kept. Today, we use search engines to look for information. According to \cite{webtables-power-2008} there are 154M tabular data they found in the web. Search Engines are performing quite well in finding textual data kept in webpages. But until today, search engine does not perform well when it comes to tabular data. They treat tabular data as textual data without taking into account the semantics hidden in the tables.

What we want to achieve is learning what does these data represents. Given enough data, we can deduce what a particular set of data represents, in other words, learns the type of this data. For example if you have the temperature for all the European countries, we will be able to tell that it is of the same type as set of temperatures of countries located in the American continent, both are temperatures of countries . Another example would be salaries for employees in Spain, we will be able to know that a set of numbers (e.g. salaries for employees in France) represent the salaries for employees. 

To reach this kind of learning that is done by computers, we use \textit{Semantic Web} to get data and their meaning, \textit{Machine Learning} for the learning process and statistical tests to be used by machine learning to distinguish a set of data from another. We will be focusing on numerical data, but this work can be extended to include categorical and textual data. In this work, we will not consider the relation between columns, but that can be an extension to the work. 

\comm{I might need to talk about data integration here}

\subsection{Problem Statement}
It is common for tables have missing headers, or headers that can\rq t be used as mentioned in \cite{webtables-power-2008} (e.g. if a column has the header "from", we don\rq t know if it is a time (a bus timetable) or a location (manufacturing countries)). So, we will be relying on the data in the column regardless of the header. 

Data Integration is also a common problem. Integrating multiple data sources can be a cumbersome as each data source can be in a different format. In the semantic web community, people use RDF (Resource Description Framework) and RDF Schema (RDFS) which is a recommendation of W3C. A lot of companies and organizations want to share their data as RDF(S), but they have their data as csv (comma separated values), excel, pdf, ...etc. Mapping the data manually is not scalable and prune to human error. It also requires domain experts to do the mapping and they need to have understanding of the semantic web and ontologies, or they need to have software engineers to help them with the mappings and transformation of the data to RDF(S).

In our work, we automatically identify/learn the types of the columns of the data sources (e.g. csv, excel) with the types being the one used in the semantic web (e.g. from DBpedia\cite{dbpedia-site}).


\subsection{Metrics}



\section{Analysis (2-4 pages)}

\subsection{Data Exploration}
loupe

\subsection{Exploratory Visualization}
\subsection{Algorithms and Techniques}

We will be performing statistical tests for each column in for each dataset. Then, we will reduce the dimensionality using PCA. After the dimensionality reduction, we use soft (fuzzy) clustering, more specifically \comm{Gaussian Mixture Model} \comm{reference}. This will result in each column in the data sets to be presented in a point in an n-dimension space and located in cluster. Since we are using soft clustering, each of the points will belong to a cluster with a probability between 0 and 1. 

  


We use soft clustering for

\subsection{Benchmark}\label{benchmark-sec}


\section{Methodology (3-5 pages)}
We will be using quantitative research methodology in our work and compare it to the benchmark mentioned in section \ref{benchmark-sec}. We will compare the precision and recall with the same data.

\subsection{Data Preprocessing}
There are two sources of data that we will be using for the learning process, the first source is a SPARQL endpoint \cite{w3c-sparql} (we will be using dbpedia) and the second data source is a csv file (we can use excel or pdf as well). 

For the SPARQL endpoint, we need to query the endpoint to get the data as tabular format to apply the statistical tests and compare it with the second source. So we query every concept that we have in the mapping file \comm{I should explain it or add a reference to it} using SPARQL (see figure \ref{fig:sparql}) with the \textit{concept} and \textit{subject} as strings (e.g. for the concept to be "" and for the subject to be "<http://dbpedia.org/property/temperature>") \comm{detailed concepts and subjects will be filled in later} using \cite{loupe} to find concepts and their properties as explained in the data exploration section. After that, we have each property of a concept as a column of data, so we output it as csv file.

For the second data source, if it was not in csv file, we convert it to csv format. This is straight forward step the we are assuming that the second data source to be in a tabular format (e.g. excel).
 



\begin{figure}
  \caption{SPARQL example}
  \label{fig:sparql}
\begin{lstlisting}[language=SPARQL]
SELECT ?object where {
        ?x a <Concept> .
        ?x  <Subject> ?object .
        }
\end{lstlisting}
\end{figure}


\subsection{Implementation}
\subsection{Refinement}


\section{Results (2-3 pages)}
\subsection{Model Evaluation and Validation}
\subsection{Justification}

\section{Conclusion (1-2 pages)}
 \subsection{Free-Form Visualization}
 \subsection{Reflection}
 \subsection{Improvement}
 
 
\begin{thebibliography}{99}
\bibitem{webtables-power-2008}Cafarella, Michael J., et al. "Webtables: exploring the power of tables on the web." Proceedings of the VLDB Endowment 1.1 (2008): 538-549.

\bibitem{dbpedia-site}
http://wiki.dbpedia.org/

\bibitem{w3c-sparql}
https://www.w3.org/wiki/SPARQL

\bibitem{loupe}
http://loupe.linkeddata.es/
\end{thebibliography}
 
 
\end{document}
